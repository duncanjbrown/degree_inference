{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454a41dd-f032-4364-9771-d21c191eb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from degree_inference.cah_data import CAHData\n",
    "from degree_inference.predict import predict\n",
    "from degree_inference.train import train\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "32ef18d3-ffb4-4c50-a118-69953c595923",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CAHData(include_ilr=True, include_gpt_inferences=False, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "86867561-419e-4a8c-8dd2-0ad3d155dbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'CAH3 codes with this many examples')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "counts = c.df['label'].value_counts()\n",
    "plt.hist(counts, bins=15)\n",
    "plt.title(\"Distibution of training examples\")\n",
    "plt.xlabel(\"# of examples\")\n",
    "plt.ylabel(\"CAH3 codes with this many examples\")\n",
    "# counts, bins = np.histogram(counts)\n",
    "# plt.stairs(counts, bins)\n",
    "# counts.plot(kind='bar').set(xlabel=None)\n",
    "# ax = plt.gca()\n",
    "# ax.get_xaxis().set_visible(False)\n",
    "# plt.title(\"Frequency of CAH3 codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebb94eaf-abd8-4cb6-8d1a-8d9375e1016e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d4ab67023942b4ad819ea47765ab40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7d7dfe378a47da86d12a700937a15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery degree_name_to_hecos\n",
    "SELECT\n",
    "  LOWER(cq.subject) AS text,\n",
    "  cq.degree_subject_cah_l3 as label\n",
    "FROM\n",
    "  `rugged-abacus-218110.dataform_ABS_2_dev.application_choice_details`\n",
    "LEFT JOIN\n",
    "  UNNEST(candidate_qualifications) AS cq\n",
    "LEFT JOIN `rugged-abacus-218110.dfe_reference_data.cah_categories_l3_v2` AS cah_codes ON cah_codes.id = degree_subject_cah_l3\n",
    "WHERE degree_level IS NOT NULL AND degree_level !='unknown'\n",
    "AND degree_subject_cah_l3 IS NOT NULL\n",
    "GROUP BY \n",
    "text,\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb9f54ff-de82-4dc0-98da-7662d9862b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/duncan/projects/transformers/.venv/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5344' max='5344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5344/5344 16:12, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.607100</td>\n",
       "      <td>4.542198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.039500</td>\n",
       "      <td>4.029944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.745000</td>\n",
       "      <td>3.665502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.318200</td>\n",
       "      <td>3.339688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.974400</td>\n",
       "      <td>3.068465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.810100</td>\n",
       "      <td>2.832941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.470200</td>\n",
       "      <td>2.639329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.305900</td>\n",
       "      <td>2.489846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.035000</td>\n",
       "      <td>2.357273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.070600</td>\n",
       "      <td>2.241990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.017000</td>\n",
       "      <td>2.155206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.794400</td>\n",
       "      <td>2.068902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.829500</td>\n",
       "      <td>1.991093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.680400</td>\n",
       "      <td>1.924799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.368300</td>\n",
       "      <td>1.879031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.560600</td>\n",
       "      <td>1.832291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.301800</td>\n",
       "      <td>1.793005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.456400</td>\n",
       "      <td>1.744444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.273600</td>\n",
       "      <td>1.723871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.201300</td>\n",
       "      <td>1.699720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.194900</td>\n",
       "      <td>1.686323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.234700</td>\n",
       "      <td>1.672558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.217800</td>\n",
       "      <td>1.659924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.093000</td>\n",
       "      <td>1.646328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.000200</td>\n",
       "      <td>1.640591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.104500</td>\n",
       "      <td>1.632083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/duncan/projects/transformers/.venv/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8808' max='8808' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8808/8808 34:52, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.535900</td>\n",
       "      <td>4.453227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.232400</td>\n",
       "      <td>4.072093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.816100</td>\n",
       "      <td>3.629062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.398900</td>\n",
       "      <td>3.244115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.107700</td>\n",
       "      <td>2.989307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.656000</td>\n",
       "      <td>2.734499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.827100</td>\n",
       "      <td>2.533079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.444000</td>\n",
       "      <td>2.395833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.310500</td>\n",
       "      <td>2.252913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.991300</td>\n",
       "      <td>2.131483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.921800</td>\n",
       "      <td>2.040335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>1.959099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.834200</td>\n",
       "      <td>1.870688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.569300</td>\n",
       "      <td>1.800418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.666500</td>\n",
       "      <td>1.746663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.413400</td>\n",
       "      <td>1.707611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.524600</td>\n",
       "      <td>1.663536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.395200</td>\n",
       "      <td>1.612881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.365800</td>\n",
       "      <td>1.579447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.157500</td>\n",
       "      <td>1.548267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.387100</td>\n",
       "      <td>1.536129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.145100</td>\n",
       "      <td>1.498006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.172000</td>\n",
       "      <td>1.495170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.106700</td>\n",
       "      <td>1.448975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.115600</td>\n",
       "      <td>1.435102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.925400</td>\n",
       "      <td>1.435210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.081000</td>\n",
       "      <td>1.409026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.978500</td>\n",
       "      <td>1.393458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.932600</td>\n",
       "      <td>1.391440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.929600</td>\n",
       "      <td>1.399497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.011700</td>\n",
       "      <td>1.383450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.873500</td>\n",
       "      <td>1.362057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.912300</td>\n",
       "      <td>1.357727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.806800</td>\n",
       "      <td>1.348898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.834700</td>\n",
       "      <td>1.335066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.946500</td>\n",
       "      <td>1.343508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.961700</td>\n",
       "      <td>1.334573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.340328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.836300</td>\n",
       "      <td>1.340202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>1.337017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.863200</td>\n",
       "      <td>1.331512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.804200</td>\n",
       "      <td>1.335615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.808700</td>\n",
       "      <td>1.331811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.790200</td>\n",
       "      <td>1.330829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/duncan/projects/transformers/.venv/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14201' max='17616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14201/17616 59:03 < 14:12, 4.01 it/s, Epoch 9.67/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.568500</td>\n",
       "      <td>4.515294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.369300</td>\n",
       "      <td>4.073011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.779200</td>\n",
       "      <td>3.678241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.441500</td>\n",
       "      <td>3.298954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.201200</td>\n",
       "      <td>2.972135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.971400</td>\n",
       "      <td>2.729782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.461900</td>\n",
       "      <td>2.517050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.432300</td>\n",
       "      <td>2.358477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.364200</td>\n",
       "      <td>2.238315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.143000</td>\n",
       "      <td>2.099734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.179300</td>\n",
       "      <td>1.997244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.798300</td>\n",
       "      <td>1.893170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.699100</td>\n",
       "      <td>1.826159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.605000</td>\n",
       "      <td>1.770258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.580500</td>\n",
       "      <td>1.736395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.438600</td>\n",
       "      <td>1.690125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.405200</td>\n",
       "      <td>1.603308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.147700</td>\n",
       "      <td>1.574349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.276500</td>\n",
       "      <td>1.530496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.228000</td>\n",
       "      <td>1.516438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.269100</td>\n",
       "      <td>1.477055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.329400</td>\n",
       "      <td>1.450243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.974800</td>\n",
       "      <td>1.408622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.971200</td>\n",
       "      <td>1.392348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.047000</td>\n",
       "      <td>1.354771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.840300</td>\n",
       "      <td>1.343143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.975900</td>\n",
       "      <td>1.314422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.821700</td>\n",
       "      <td>1.329431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.724200</td>\n",
       "      <td>1.319365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.852800</td>\n",
       "      <td>1.329543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.680900</td>\n",
       "      <td>1.285223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>1.287841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.753500</td>\n",
       "      <td>1.294187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.798100</td>\n",
       "      <td>1.265660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.755700</td>\n",
       "      <td>1.263694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.700200</td>\n",
       "      <td>1.246887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.643200</td>\n",
       "      <td>1.260171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>1.248648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.609700</td>\n",
       "      <td>1.249606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.576600</td>\n",
       "      <td>1.213071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.436500</td>\n",
       "      <td>1.225951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.643300</td>\n",
       "      <td>1.240062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.563200</td>\n",
       "      <td>1.213151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.675700</td>\n",
       "      <td>1.202496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.532200</td>\n",
       "      <td>1.184181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.449100</td>\n",
       "      <td>1.182433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.543900</td>\n",
       "      <td>1.199601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.529100</td>\n",
       "      <td>1.193537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.518500</td>\n",
       "      <td>1.214052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.440200</td>\n",
       "      <td>1.204597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.379200</td>\n",
       "      <td>1.206771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>1.210668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.460100</td>\n",
       "      <td>1.195290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>1.212939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>1.228492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.514700</td>\n",
       "      <td>1.203745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.320300</td>\n",
       "      <td>1.224054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>1.213198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.341100</td>\n",
       "      <td>1.216769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>1.230271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.299900</td>\n",
       "      <td>1.210242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.342500</td>\n",
       "      <td>1.219079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>1.222552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>1.190277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.355900</td>\n",
       "      <td>1.217625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.281600</td>\n",
       "      <td>1.212180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.345600</td>\n",
       "      <td>1.225747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.311500</td>\n",
       "      <td>1.230551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.319700</td>\n",
       "      <td>1.230634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.325200</td>\n",
       "      <td>1.233984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 61/184 00:01 < 00:03, 36.20 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m mps_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mto(mps_device)\n\u001b[0;32m---> 34\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcah-ilr-gpt-augmented\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/cah-ilr-gpt-augmented\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/transformers/degree_inference/train.py:48\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, learning_rate, epochs, batch_size, comment)\u001b[0m\n\u001b[1;32m     39\u001b[0m datasets \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdatasets()\n\u001b[1;32m     41\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m     42\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     43\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     44\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdatasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     45\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mdatasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     46\u001b[0m )\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m best_checkpoint \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_checkpoint)\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/transformers/trainer.py:1901\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1898\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1902\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1903\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/transformers/trainer.py:2226\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2224\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2226\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2229\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/transformers/trainer.py:2934\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2931\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2933\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2934\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2935\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2937\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2938\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2942\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2944\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/transformers/trainer.py:3123\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3120\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3122\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3123\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3124\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/transformers/trainer.py:3337\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   3336\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3337\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3338\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   3340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/projects/transformers/degree_inference/train.py:12\u001b[0m, in \u001b[0;36mtrain.<locals>.CustomTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     10\u001b[0m     labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(data\u001b[38;5;241m.\u001b[39mclass_weights()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m), device\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice))\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:355\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    352\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[0;32m~/projects/transformers/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1813\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1809\u001b[0m         ret \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m-> 1813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(\u001b[38;5;28minput\u001b[39m: Tensor, dim: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, _stacklevel: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, dtype: Optional[DType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m   1814\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies a softmax function.\u001b[39;00m\n\u001b[1;32m   1815\u001b[0m \n\u001b[1;32m   1816\u001b[0m \u001b[38;5;124;03m    Softmax is defined as:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1836\u001b[0m \n\u001b[1;32m   1837\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = CAHData(include_ilr=False, augment=False, include_gpt_inferences=False)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(data.df['label'].unique()))\n",
    "mps_device = torch.device(\"mps\")\n",
    "model.to(mps_device)\n",
    "\n",
    "trainer = train(model,data,epochs=30,comment=\"cah\")\n",
    "\n",
    "model.save_pretrained(\"./models/cah\")\n",
    "\n",
    "data = CAHData(include_ilr=True, augment=False, include_gpt_inferences=False)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(data.df['label'].unique()))\n",
    "mps_device = torch.device(\"mps\")\n",
    "model.to(mps_device)\n",
    "\n",
    "trainer = train(model,data,epochs=16,comment=\"cah-ilr\")\n",
    "model.save_pretrained(\"./models/cah-ilr\")\n",
    "# model.to(\"cpu\")\n",
    "\n",
    "data = CAHData(include_ilr=True, augment=False, include_gpt_inferences=True)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(data.df['label'].unique()))\n",
    "mps_device = torch.device(\"mps\")\n",
    "model.to(mps_device)\n",
    "\n",
    "trainer = train(model,data,epochs=12,comment=\"cah-ilr-gpt\")\n",
    "\n",
    "model.save_pretrained(\"./models/cah-ilr-gpt\")\n",
    "\n",
    "data = CAHData(include_ilr=True, augment=True, include_gpt_inferences=True)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(data.df['label'].unique()))\n",
    "mps_device = torch.device(\"mps\")\n",
    "model.to(mps_device)\n",
    "\n",
    "trainer = train(model,data,epochs=12,comment=\"cah-ilr-gpt-augmented\")\n",
    "\n",
    "model.save_pretrained(\"./models/cah-ilr-gpt-augmented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3dedf0-0191-4fe1-ac60-162b1bfd3a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x36c3a11c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = predict(data, model,list(degree_name_to_hecos['text']))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a31e7783-e540-4291-a3e1-95d5b62f250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./models/cah-ilr-gpt-augmented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea2d9f5-cd7a-4a73-b607-4fced868d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./cah-ilr-gpt-augmented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8b7ad1a-fc48-4b2f-916f-1d12aa05a1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['03-02-01', '03-01-10', '15-04-03', '10-01-03', '15-01-02',\n",
       "       '17-01-09', '15-01-03', '19-04-01', '20-02-02', '07-04-04',\n",
       "       '15-04-01', '19-01-02', '19-02-04', '25-01-03', '25-01-04',\n",
       "       '09-01-01', '22-01-02', '02-05-02', '15-02-01', '10-01-07',\n",
       "       '10-01-08', '15-03-01', '24-01-05', ' 20-01-01', '11-01-02',\n",
       "       '11-01-06', '17-01-07', '23-01-01', ' 22-01-01', ' 01-01-01',\n",
       "       '19-01-03', '19-04-09', '26-01-01', '22-01-01', '25-02-03',\n",
       "       '17-01-08', '11-01-05', '25-01-02', '20-01-01', '17-01-04',\n",
       "       ' 15-04-01', '26-01-04', '17-01-01', '03-01-03', '03-01-06',\n",
       "       ' 25-01-04', '25-02-04', '25-02-02', '06-01-04', '19-04-05',\n",
       "       '11-01-04', '25-02-01', '07-01-01', '24-01-04', '01-01-02',\n",
       "       '02-02-01', '19-04-08', '19-01-05', '04-01-01', '16-01-01',\n",
       "       '25-01-01', '04-01-04', '20-01-03', '17-01-02', '19-04-04',\n",
       "       '25-01-05', '03-01-08', '11-01-01', '07-02-01', '04-01-02',\n",
       "       '26-01-03', '26-01-06', '10-01-09', ' 26-01-03', '04-01-05',\n",
       "       '20-02-01', '17-01-06', '09-01-03', '19-01-06', ' 25-02-03',\n",
       "       '10-01-02', '02-02-03', '06-01-03', '11-01-03', '17-01-05',\n",
       "       ' 11-01-07', '03-01-05', '06-01-08', '02-06-02', '02-04-05',\n",
       "       '11-01-07', '15-04-02', '10-01-10', '05-01-02', '15-01-05',\n",
       "       '01-01-03', '10-03-01', '19-01-01', ' 19-01-03', '17-01-03',\n",
       "       '19-04-07', '02-04-07', '10-01-06', '15-01-01', '06-01-01',\n",
       "       ' 02-05-04', '03-01-01', '10-03-06', '13-01-02', '13-01-01',\n",
       "       '07-01-02', '19-01-04', '20-01-04', ' 19-01-02', '03-01-02',\n",
       "       '09-01-02', ' 03-02-01', '06-01-07', '02-04-01', '02-06-07',\n",
       "       '20-01-02', ' 25-02-02', '07-04-02', '01-01-01', '19-01-07',\n",
       "       '06-01-05', '07-04-03', '23-01-02', '02-05-03', '19-04-02',\n",
       "       '26-01-02', '03-01-07', '05-01-01', '02-06-01', ' 25-01-03',\n",
       "       '06-01-02', '02-06-04', '02-06-05', '19-02-02', '10-01-04',\n",
       "       '10-01-01', '10-01-05', '11-01-08', '02-05-04', '10-03-03',\n",
       "       '19-04-06', '26-01-05', '13-01-04', ' 19-04-01', '15-01-04',\n",
       "       '03-01-04', '15-01-06', '20-01-05', '02-04-09', '04-01-03',\n",
       "       ' 15-04-02', '02-04-03', ' 17-01-01', ' 26-01-01', ' 25-01-02',\n",
       "       '13-01-03', ' 07-04-03', '10-03-07', ' 20-01-05', '19-02-03',\n",
       "       '02-04-06', '24-01-03', ' 19-04-06', ' 15-01-02', ' 25-02-04',\n",
       "       '02-06-03', ' 15-02-01', '24-01-01', '02-04-02', ' 07-04-04',\n",
       "       '01-01-04', ' 23-01-01', ' 11-01-01', ' 09-01-01', ' 11-01-02',\n",
       "       ' 03-01-01', ' 19-04-09', '10-03-04', '02-06-06', '03-01-09',\n",
       "       ' 03-01-07', '24-01-02', '10-03-05', '23-01-03', '23-01-04',\n",
       "       'label', '19-04-03', ' 20-01-02', ' 15-03-01', '02-02-02',\n",
       "       '02-05-01', ' 19-01-01', ' 26-01-04', ' 15-01-03', ' 17-01-03',\n",
       "       '06-01-06', ' 19-01-06', ' 04-01-01', ' 19-02-01', ' 07-04-02',\n",
       "       '19-02-01', ' 11-01-03', ' 25-02-01', ' 22-01-02', '02-04-08',\n",
       "       '07-04-01', ' 02-05-02', ' 20-02-01', ' 04-01-02', ' 25-01-01',\n",
       "       ' 19-01-07', ' 24-01-01', ' 07-04-01', ' 24-01-05', ' 10-03-04',\n",
       "       '02-04-04', ' 06-01-02', '10-03-02', ' 16-01-01', ' 17-01-06',\n",
       "       ' 07-01-01', ' 19-01-05', ' 17-01-05', ' 20-02-02', ' 15-01-05',\n",
       "       ' 03-01-02', ' 19-02-04', ' 03-01-03', ' 03-01-08', ' 07-02-01',\n",
       "       ' 20-01-03', ' 23-01-03', ' 19-04-03', ' 07-01-02', ' 24-01-02',\n",
       "       ' 19-04-08'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.df['label'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85e85f67-552e-4b74-9fba-d48f599f97a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd8c2a974074f889dfb8c6dcf796099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c48f57a868e4c9f83a5903176bc42e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery unlabelled\n",
    "SELECT\n",
    "        cq.subject AS degree_subject,\n",
    "        cq.degree_subject_cah_l3,\n",
    "        cah_codes.name AS cah_category_name\n",
    "    FROM\n",
    "        `rugged-abacus-218110.dataform_ABS_2_dev.application_choice_details`\n",
    "    LEFT JOIN\n",
    "        UNNEST(candidate_qualifications) AS cq\n",
    "    LEFT JOIN `rugged-abacus-218110.dfe_reference_data.cah_categories_l3_v2` AS cah_codes ON cah_codes.id = degree_subject_cah_l3\n",
    "    WHERE degree_level IS NOT NULL AND degree_level !='unknown'\n",
    "    AND nationality_group = \"British\"\n",
    "    AND degree_subject_cah_l3 IS NULL\n",
    "    GROUP BY\n",
    "        degree_subject,\n",
    "        degree_subject_cah_l3,\n",
    "        cah_category_name\n",
    "    ORDER BY RAND()\n",
    "    LIMIT 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acd7c309-5b43-4b6c-b135-bda77738332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Single Honours Psychology', '04-01-02', 'applied psychology')\n",
      "('BA Urban Regeneration and Planning', '13-01-04', 'planning (urban, rural and regional)')\n",
      "('Molecular science', '03-01-08', 'molecular biology, biophysics and biochemistry')\n",
      "('History and geography', '26-01-01', 'geography (non-specific)')\n",
      "('Mathematical Sciences (Environmental Sciences)', '09-01-01', 'mathematics')\n",
      "('Economics & Statistics', '09-01-03', 'statistics')\n",
      "('Criminology and Criminal Justice ', '15-04-01', 'social work')\n",
      "('History (HL)', '20-01-01', 'history')\n",
      "('Irish History and Politics', '19-04-08', 'American and Australasian studies')\n",
      "('business management (legal regulations)', '17-01-01', 'business and management (non-specific)')\n",
      "('English & History', '19-01-01', 'English studies (non-specific)')\n",
      "('BA Turkish and History', '19-04-07', 'African and modern Middle Eastern studies')\n",
      "('Broadcast Journalism', '24-01-04', 'journalism')\n",
      "('Music and Hispanic Studies', '25-02-02', 'music')\n",
      "('Bsc (Hons) Television Technology And Production', '11-01-06', 'computer games and animation')\n",
      "('Economics and Social Studies', '15-01-01', 'social sciences (non-specific)')\n",
      "('International History and Politics', '15-01-01', 'social sciences (non-specific)')\n",
      "('Hispanic Culture and Languages', '19-04-04', 'Iberian studies')\n",
      "('Business Studies, Economics, Accountancy', '17-01-01', 'business and management (non-specific)')\n",
      "('International Relations (Middle East)', '15-03-01', 'politics')\n",
      "('History and English Literature', '20-01-01', 'history')\n",
      "('Architectural studies', '13-01-01', 'architecture')\n",
      "('Philosophy.', '20-02-01', 'philosophy')\n",
      "('Professional Dance & Musical Theatre', '25-02-04', 'dance')\n",
      "('Documentary Journalism', '24-01-04', 'journalism')\n",
      "('Psychotherapy', '02-06-07', 'counselling, psychotherapy and occupational therapy')\n",
      "('Photography in the Arts', '25-01-04', 'cinematics and photography')\n",
      "('Further maths', '09-01-01', 'mathematics')\n",
      "('International Business and Law ', '17-01-01', 'business and management (non-specific)')\n",
      "('Applied Custodial Leadership', '17-01-05', 'human resource management')\n",
      "('Education and Professional Training  Levels 5 and 6', '22-01-02', 'teacher training')\n",
      "('Genomic Medicine and Healthcare', '01-01-01', 'medical sciences (non-specific)')\n",
      "('Youth and Community work with Applied Theology', '15-04-02', 'childhood and youth studies')\n",
      "('English literature and Primary QTS', '19-01-03', 'literature in English')\n",
      "('Social Welfare', '15-04-01', 'social work')\n",
      "('Photography, Video and Digital Imaging', '25-01-04', 'cinematics and photography')\n",
      "('Criminology and English Language', '15-03-01', 'politics')\n",
      "('Fine Art with Museum and Gallery Studies', '25-01-02', 'art')\n",
      "('Animal behaviour, welfare and conservation', '06-01-01', 'animal science')\n",
      "('Law with business management', '16-01-01', 'law')\n",
      "('Economic', '15-02-01', 'economics')\n",
      "('Masters of Law (LLM)', '16-01-01', 'law')\n",
      "('Sport Studies ', '03-02-01', 'sport and exercise sciences')\n",
      "('Performance Theatre', '25-02-03', 'drama')\n",
      "('Technical theatre studies- Costume', '25-02-03', 'drama')\n",
      "('Physics chemistry biology', '03-01-02', 'biology (non-specific)')\n",
      "('Ecotourism and Countryside Management', '26-01-02', 'physical geographical sciences')\n",
      "('Level 5 FdEd Childhood Studies', '15-04-02', 'childhood and youth studies')\n",
      "('Education studies and Early Childhood', '22-01-01', 'education')\n",
      "('Communication Arts', '25-01-01', 'creative arts and design (non-specific)')\n",
      "('Physics and Philosophy', '07-01-01', 'physics')\n",
      "('Social Media and Digital Marketing', '24-01-02', 'publicity studies')\n",
      "('Physical Education with Dance', '25-02-04', 'dance')\n",
      "('CIMA', '23-01-01', 'combined, general or negotiated studies')\n",
      "('Psychology with Clinical psychology', '04-01-02', 'applied psychology')\n",
      "('Mathematics with Business Management', '09-01-01', 'mathematics')\n",
      "('CertPT Early Childhood Education', '22-01-01', 'education')\n",
      "('Sociology and International Relations', '15-01-02', 'sociology')\n",
      "('History with Social Sciences', '20-01-01', 'history')\n",
      "(\"Children's Nursing\", '02-04-09', 'others in nursing')\n",
      "('Fine Art with Diploma of International Studies', '25-01-02', 'art')\n",
      "('Fine Arts and Humanities (Visual Arts and Architecture)', '25-01-01', 'creative arts and design (non-specific)')\n",
      "('Sports studies and History', '03-02-01', 'sport and exercise sciences')\n",
      "('History and archaeology', '20-01-03', 'archaeology')\n",
      "('Eighteenth century studies', '19-01-03', 'literature in English')\n",
      "('Applied Psychology and criminology', '04-01-02', 'applied psychology')\n",
      "('Applied Community Studies', '15-01-01', 'social sciences (non-specific)')\n",
      "('English (Higher)', '19-01-02', 'English language')\n",
      "('Modern Language Studies with English & Spanish', '19-02-02', 'Gaelic studies')\n",
      "('Sports performance and coaching', '03-02-01', 'sport and exercise sciences')\n",
      "('Business Organisation & Management', '17-01-01', 'business and management (non-specific)')\n",
      "('Film and TV production', '25-01-04', 'cinematics and photography')\n",
      "('Design For Communication Media (Graphics)', '25-01-03', 'design studies')\n",
      "('Education in Early Childhood', '22-01-01', 'education')\n",
      "('Furniture & Product Design', '25-01-03', 'design studies')\n",
      "('FINANCIAL MANAGEMENT', '17-01-07', 'finance')\n",
      "('Terrorism and Criminology', '15-01-02', 'sociology')\n",
      "('Theatre Practice: Performance and Scenography', '25-02-03', 'drama')\n",
      "('Accounting and business management degree', '17-01-08', 'accounting')\n",
      "('Zoology, Botany, Chemistry, English', '03-01-02', 'biology (non-specific)')\n",
      "('International Human Resources', '17-01-05', 'human resource management')\n",
      "('Consumer Studies', '17-01-03', 'marketing')\n",
      "('Chinese and Russian', '19-04-06', 'Asian studies')\n",
      "('International child studies', '15-04-02', 'childhood and youth studies')\n",
      "('Conflict, Statebuilding and Development', '15-03-01', 'politics')\n",
      "('Multilingualism, Linguistics and Education', '19-01-07', 'linguistics')\n",
      "('Visual Studies with Combined Studies', '25-01-03', 'design studies')\n",
      "('Information management and security', '24-01-01', 'information services')\n",
      "('Marine biology and coastal ecology', '03-01-03', 'ecology and environmental biology')\n",
      "('Strength Conditioning & Sports Coaching', '03-02-01', 'sport and exercise sciences')\n",
      "('English as a second language', '19-01-02', 'English language')\n",
      "('PSYCHOSOCIAL STUDIES', '02-06-07', 'counselling, psychotherapy and occupational therapy')\n",
      "('Early Childhood practice', '22-01-01', 'education')\n",
      "('Dance Practice and Performance and Sport Studies', '25-02-04', 'dance')\n",
      "('Textiles In Practice', '10-03-03', 'polymers and textiles')\n",
      "('English literature and media studies', '19-01-03', 'literature in English')\n",
      "('Special educational Needs and disabilities', '22-01-01', 'education')\n",
      "('Early years childhood and education studies', '22-01-01', 'education')\n",
      "('Educational Leadership and Management', '22-01-01', 'education')\n",
      "('German with Creative Writing', '19-04-02', 'German and Scandinavian studies')\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "\n",
    "# load pretrained model\n",
    "model = BertForSequenceClassification.from_pretrained('./30-epoch-gpt2-ilr-augmented-1e-5/', num_labels=len(data.df['label'].unique()))\n",
    "model.to(\"cpu\")\n",
    "\n",
    "out = predict(data, model,list(unlabelled.degree_subject))\n",
    "for row in out:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
